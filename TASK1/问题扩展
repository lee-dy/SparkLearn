1.spark 和 mapreduce有哪些区别，请用具体的例子说明？
①Hadoop MapReduce采用了多进程模型，而Spark采用了多线程模型
②Spark中最核心的概念是RDD(弹性分布式数据集),Spark的计算模式也属于MapReduce，
但不局限于Map和Reduce操作，还提供了多种数据集操作类型，编程模型比MapReduce更灵活。
③Spark提供了内存计算，中间结果直接放到内存中，带来了更高的迭代运算效率；
④Spark基于DAG的任务调度执行机制，要优于MapReduce的迭代执行机制。
⑤使用Hadoop需要编写不少相对底层的代码，不够高效。相对而言，Spark提供了多种高层次、简洁的API，
通常情况下，对于实现相同功能的应用程序，Spark的代码量要比Hadoop少2-5倍。更重要的是，Spark提供
了实时交互式编程反馈，可以方便地验证、调整算法。

2.rdd的本质是什么？
可以将 RDD 理解为一个分布式对象集合，本质上是一个只读的分区记录集合。每个 RDD 可以分成多个分区，
每个分区就是一个数据集片段。一个 RDD 的不同分区可以保存到集群中的不同结点上，从而可以在集群中的
不同结点上进行并行计算。
